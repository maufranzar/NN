{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Parámetros globales\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 5\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones aplicadas a las imágenes\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Redimensionamos las imágenes a 128x128 píxeles\n",
    "    transforms.ToTensor(),  # Convertimos las imágenes a tensores\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalizamos los canales RGB\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir  # Directorio raíz de los datos\n",
    "        self.transform = transform  # Transformaciones a aplicar\n",
    "        self.samples = []  # Lista de rutas de las imágenes\n",
    "        self.labels = []  # Lista de etiquetas correspondientes\n",
    "\n",
    "        # Definir las clases y asignar índices numéricos\n",
    "        self.classes = ['tones', 'chords', 'melodies', 'chord_melodies', 'superposed']\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        # Cargar los datos\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        # Recorremos cada clase y cargamos las rutas de los archivos y sus etiquetas\n",
    "        for cls_name in self.classes:\n",
    "            cls_dir = os.path.join(self.data_dir, cls_name)\n",
    "            if not os.path.isdir(cls_dir):\n",
    "                continue\n",
    "            for filename in os.listdir(cls_dir):\n",
    "                if filename.endswith('_spectrogram.png'):\n",
    "                    filepath = os.path.join(cls_dir, filename)\n",
    "                    self.samples.append(filepath)\n",
    "                    self.labels.append(self.class_to_idx[cls_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        # Retorna el número total de muestras\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Carga y retorna una muestra y su etiqueta correspondiente\n",
    "        img_path = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al directorio 'data' desde el directorio actual ('notebooks/')\n",
    "data_dir = '../data'  # Ajusta la ruta según la estructura de tus directorios\n",
    "\n",
    "# Verificar el directorio de trabajo actual\n",
    "print(f\"Directorio de trabajo actual: {os.getcwd()}\")\n",
    "\n",
    "# Verificar que el directorio 'data' existe\n",
    "if not os.path.isdir(data_dir):\n",
    "    raise FileNotFoundError(f\"El directorio '{data_dir}' no existe. Verifica la ruta.\")\n",
    "else:\n",
    "    print(f\"El directorio '{data_dir}' ha sido encontrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear instancia del dataset\n",
    "dataset = AudioDataset(data_dir=data_dir, transform=transform)\n",
    "\n",
    "# Verificar que el dataset no esté vacío\n",
    "if len(dataset) == 0:\n",
    "    raise ValueError(\"El dataset está vacío. Asegúrate de que hay archivos '_spectrogram.png' en las subcarpetas.\")\n",
    "else:\n",
    "    print(f\"Número total de muestras en el dataset: {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar las muestras por clase\n",
    "class_counts = Counter()\n",
    "for label in dataset.labels:\n",
    "    class_name = dataset.classes[label]\n",
    "    class_counts[class_name] += 1\n",
    "\n",
    "print(\"Distribución de muestras por clase:\")\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"Clase '{class_name}': {count} muestras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el dataset en conjuntos de entrenamiento y validación (80% entrenamiento, 20% validación)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Fijar una semilla para reproducibilidad\n",
    "torch.manual_seed(42)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Crear DataLoaders para cargar los datos en lotes durante el entrenamiento\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Mostrar estadísticas de los conjuntos\n",
    "print(f\"Muestras de entrenamiento: {len(train_dataset)}\")\n",
    "print(f\"Muestras de validación: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener un lote de datos para verificar\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "print(f\"Tamaño del lote de imágenes: {images.shape}\")  # Debe ser [BATCH_SIZE, 3, 128, 128]\n",
    "print(f\"Tamaño del lote de etiquetas: {labels.shape}\")  # Debe ser [BATCH_SIZE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para mostrar una imagen\n",
    "def imshow(img, label):\n",
    "    img = img / 2 + 0.5  # Desnormalizamos la imagen\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(f\"Etiqueta: {dataset.classes[label]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Mostrar algunas imágenes del lote\n",
    "for i in range(4):\n",
    "    imshow(images[i], labels[i].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        \n",
    "        # Definición de la parte CNN del modelo\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),  # Capa de convolución\n",
    "            nn.BatchNorm2d(16),  # Normalización por lotes\n",
    "            nn.ReLU(),  # Función de activación\n",
    "            nn.MaxPool2d(2, 2),  # Capa de pooling\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        \n",
    "        # Parámetros para la parte LSTM\n",
    "        self.lstm_input_size = 32 * 32  # Tamaño de entrada para la LSTM\n",
    "        self.hidden_size = 64  # Tamaño de las capas ocultas de la LSTM\n",
    "        self.num_layers = 2  # Número de capas en la LSTM\n",
    "        \n",
    "        # Definición de la LSTM\n",
    "        self.lstm = nn.LSTM(input_size=self.lstm_input_size, hidden_size=self.hidden_size,\n",
    "                            num_layers=self.num_layers, batch_first=True)\n",
    "        \n",
    "        # Capa totalmente conectada para la clasificación final\n",
    "        self.fc = nn.Linear(self.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.cnn(x)  # Pasamos por la CNN\n",
    "        x = x.view(batch_size, -1, self.lstm_input_size)  # Remodelamos para la LSTM\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)  # Estado oculto inicial\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)  # Estado de celda inicial\n",
    "        out, _ = self.lstm(x, (h_0, c_0))  # Pasamos por la LSTM\n",
    "        out = out[:, -1, :]  # Tomamos la última salida de la secuencia\n",
    "        out = self.fc(out)  # Pasamos por la capa final\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una instancia del modelo\n",
    "model = CNN_LSTM(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Mover el modelo al dispositivo disponible (GPU si está disponible)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Definir la función de pérdida\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Definir el optimizador\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    # Historial para almacenar pérdidas y precisiones\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # ---------- Entrenamiento ----------\n",
    "        model.train()  # Ponemos el modelo en modo entrenamiento\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # Limpiamos los gradientes\n",
    "            outputs = model(images)  # Forward\n",
    "            loss = criterion(outputs, labels)  # Calculamos la pérdida\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Actualizamos los parámetros\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        \n",
    "        # ---------- Validación ----------\n",
    "        model.eval()  # Ponemos el modelo en modo evaluación\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():  # Desactivamos el cálculo de gradientes\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_epoch_loss = val_loss / len(val_loader.dataset)\n",
    "        val_epoch_acc = 100 * val_correct / val_total\n",
    "        history['val_loss'].append(val_epoch_loss)\n",
    "        history['val_acc'].append(val_epoch_acc)\n",
    "        \n",
    "        # Imprimimos las estadísticas de la época\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, '\n",
    "              f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.2f}%')\n",
    "    \n",
    "    return history  # Retornamos el historial para análisis posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo y almacenamos el historial\n",
    "history = train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar la pérdida durante el entrenamiento\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history['train_loss'], label='Entrenamiento')\n",
    "plt.plot(history['val_loss'], label='Validación')\n",
    "plt.title('Pérdida durante el entrenamiento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Graficar la precisión durante el entrenamiento\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history['train_acc'], label='Entrenamiento')\n",
    "plt.plot(history['val_acc'], label='Validación')\n",
    "plt.title('Precisión durante el entrenamiento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisión (%)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado para uso futuro\n",
    "model_save_path = 'cnn_lstm_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Modelo guardado en {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación adicional\n",
    "# Obtener todas las predicciones y etiquetas reales en el conjunto de validación\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Especificar todas las etiquetas posibles\n",
    "all_classes = list(range(len(dataset.classes)))  # [0, 1, 2, 3, 4]\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=all_classes)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=dataset.classes, yticklabels=dataset.classes, cmap='Blues')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Etiqueta Verdadera')\n",
    "plt.title('Matriz de Confusión en el Conjunto de Validación')\n",
    "plt.show()\n",
    "\n",
    "# Mostrar reporte de clasificación\n",
    "print(\"Reporte de Clasificación:\")\n",
    "print(classification_report(all_labels, all_preds, labels=all_classes, target_names=dataset.classes, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "# Mover el modelo a CPU si está en GPU\n",
    "model_cpu = model.to('cpu')\n",
    "\n",
    "# Obtener el resumen del modelo\n",
    "print(\"Resumen del Modelo:\")\n",
    "summary(model_cpu, input_size=(BATCH_SIZE, 3, 128, 128))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
