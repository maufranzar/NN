{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importancion de Librerías y Definición de Parámetros\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchviz import make_dot\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Parámetros globales\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 10  # Número de clases para la clasificación (ajusta según tus necesidades)\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definicion de Dataset personalizado\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Directorio con las imágenes de espectrogramas.\n",
    "            transform (callable, optional): Transformaciones que se aplicarán a los datos.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Cargar los datos\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        # Asumiendo que las imágenes de espectrogramas están en subcarpetas según la clase\n",
    "        classes = os.listdir(self.data_dir)\n",
    "        class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "\n",
    "        for cls_name in classes:\n",
    "            cls_dir = os.path.join(self.data_dir, cls_name)\n",
    "            for filename in os.listdir(cls_dir):\n",
    "                if filename.endswith('.png'):\n",
    "                    filepath = os.path.join(cls_dir, filename)\n",
    "                    self.samples.append(filepath)\n",
    "                    self.labels.append(class_to_idx[cls_name])\n",
    "\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        import PIL.Image as Image\n",
    "\n",
    "        img_path = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir transformaciones\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Ajusta el tamaño según sea necesario\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear Instancias del Dataset y del DataLoader\n",
    "\n",
    "# Directorio donde se encuentran los espectrogramas\n",
    "data_dir = 'data/spectrograms'  # Ajusta esta ruta a donde estén tus datos\n",
    "\n",
    "# Crear datasets\n",
    "dataset = AudioDataset(data_dir=data_dir, transform=transform)\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y validación\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Crear DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion de la Arquitectura del Modelo\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        \n",
    "        # Capas convolucionales\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),  # 3 canales RGB\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Reduce las dimensiones a la mitad\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        \n",
    "        # Parámetros para LSTM\n",
    "        self.lstm_input_size = 32 * 32  # Ajusta según las dimensiones después de las capas CNN\n",
    "        self.hidden_size = 64\n",
    "        self.num_layers = 2\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(input_size=self.lstm_input_size, hidden_size=self.hidden_size,\n",
    "                            num_layers=self.num_layers, batch_first=True)\n",
    "        \n",
    "        # Capa totalmente conectada\n",
    "        self.fc = nn.Linear(self.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # Aplicar CNN\n",
    "        x = self.cnn(x)  # Salida: [batch_size, canales, altura, anchura]\n",
    "        \n",
    "        # Preparar los datos para LSTM\n",
    "        x = x.view(batch_size, -1, self.lstm_input_size)  # Salida: [batch_size, secuencia, características]\n",
    "        \n",
    "        # LSTM\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, (hn, cn) = self.lstm(x, (h_0, c_0))  # out: [batch_size, secuencia, hidden_size]\n",
    "        \n",
    "        # Tomar la última salida de la secuencia\n",
    "        out = out[:, -1, :]  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Capa totalmente conectada\n",
    "        out = self.fc(out)  # [batch_size, num_classes]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del Modelo\n",
    "\n",
    "# Crear instancia del modelo\n",
    "model = CNN_LSTM(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Mover el modelo a la GPU si está disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion de Entrenamiento\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Adelante\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Atrás y optimización\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Estadísticas\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_epoch_loss = val_loss / len(val_loader.dataset)\n",
    "        val_epoch_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, '\n",
    "              f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# Crear una muestra de entrada\n",
    "dummy_input = torch.randn(1, 3, 128, 128).to(device)  # Ajusta las dimensiones según sea necesario\n",
    "\n",
    "# Obtener la salida del modelo\n",
    "model.eval()\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Generar y guardar la imagen\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "dot.format = 'png'\n",
    "dot.render('model_architecture')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Instalar torchsummary si no lo tienes\n",
    "# pip install torchsummary\n",
    "\n",
    "summary(model, input_size=(3, 128, 128))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
